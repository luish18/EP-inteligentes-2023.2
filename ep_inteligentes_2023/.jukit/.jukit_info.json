{"cmd": "from ep_inteligentes_2023.io import leImagens\nimport torch\nfrom torch import nn\nfrom icecream import ic\nfrom typing import Literal\nimport torchmetrics as tmetrics\nfrom tqdm import tqdm\nfrom memory_profiler import profile\n\n\nclass dataset(torch.utils.data.Dataset):\n    def __init__(self, dataset_type: Literal[\"Train\", \"Test\"]):\n        if dataset_type == \"Test\":\n            paths = [\n                \"./data/Lung Segmentation Data/Lung Segmentation Data/Test/COVID-19/images/*.png\",\n                \"./data/Lung Segmentation Data/Lung Segmentation Data/Test/Non-COVID/images/*.png\",\n                \"./data/Lung Segmentation Data/Lung Segmentation Data/Test/Normal/images/*.png\",\n                \"./data/Lung Segmentation Data/Lung Segmentation Data/Val/COVID-19/images/*.png\",\n                \"./data/Lung Segmentation Data/Lung Segmentation Data/Val/Non-COVID/images/*.png\",\n                \"./data/Lung Segmentation Data/Lung Segmentation Data/Val/Normal/images/*.png\",\n            ]\n\n            self.imgs, self.labels = leImagens(\n                paths,\n                classes=[0, 1, 2, 0, 1, 2],\n            )\n\n        else:\n            paths = [\n                \"./data/Lung Segmentation Data/Lung Segmentation Data/Train/COVID-19/images/*.png\",\n                \"./data/Lung Segmentation Data/Lung Segmentation Data/Train/Non-COVID/images/*.png\",\n                \"./data/Lung Segmentation Data/Lung Segmentation Data/Train/Normal/images/*.png\",\n            ]\n\n            self.imgs, self.labels = leImagens(\n                paths,\n                classes=[0, 1, 2],\n            )\n\n    def __len__(self):\n        return self.labels.shape[0]\n\n    def __getitem__(self, index):\n        return torch.tensor(self.imgs), torch.tensor(self.labels)\n\n\nclass Network(nn.Module):\n    def __init__(self, k_size=(3, 3)):\n        super().__init__()\n\n        self.model = nn.Sequential(\n            nn.Conv2d(1, 128, kernel_size=k_size),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(num_features=128),\n            nn.Conv2d(128, 64, kernel_size=k_size),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(num_features=64),\n            nn.Conv2d(64, 32, kernel_size=k_size),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(num_features=32),\n            nn.Conv2d(32, 16, kernel_size=k_size),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(num_features=16),\n            nn.Conv2d(16, 32, kernel_size=k_size),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(num_features=32),\n            nn.Conv2d(32, 64, kernel_size=k_size),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(num_features=64),\n            nn.Conv2d(64, 128, kernel_size=k_size),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(num_features=128),\n            nn.Flatten(),\n            nn.Linear(128 * 224 * 224, 64),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.2),\n            nn.Linear(64, 32),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.2),\n            nn.Linear(32, 16),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.2),\n            nn.Linear(16, 3),\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n@profile\ndef train_loop(\n    dataloader: torch.utils.data.DataLoader,\n    model: nn.Module,\n    loss_fn: nn.Module,\n    optimizer,\n    metrics: tmetrics.MetricCollection | tmetrics.MetricTracker,\n    device: torch.device,\n):\n    model.train()\n    metrics.train()\n    metrics.increment()\n    running_loss = 0\n\n    iterator = tqdm(dataloader, total=len(dataloader.dataset), leave=False)\n\n    for features, labels in iterator:\n        features.to(device)\n        labels.to(device)\n\n        pred = model(features)\n\n        loss = loss_fn(pred, labels)\n        running_loss += loss.item()\n\n        metrics.update(torch.squeeze(pred.item()), torch.squeeze(labels.item()))\n        update = metrics.compute()\n        update.update({\"train/loss\": loss.item()})\n\n        loss.backward()\n\n        optimizer.step()\n        optimizer.zero_grad()\n\n    return running_loss / len(dataloader.dataset)\n\n\ndef test_loop(\n    dataloader: torch.utils.data.DataLoader,\n    model: nn.Module,\n    loss_fn: nn.Module,\n    metrics: tmetrics.MetricCollection | tmetrics.MetricTracker,\n    device: torch.device,\n):\n    model.eval()\n    metrics.eval()\n    metrics.increment()\n\n    running_loss = 0\n    iterator = tqdm(dataloader, leave=False)\n\n    with torch.no_grad():\n        for features, labels in iterator:\n            pred = model(features)\n\n            loss = loss_fn(pred, labels)\n            running_loss += loss.item()\n\n            metrics.update(torch.squeeze(pred.item()), torch.squeeze(labels.item()))\n            update = metrics.compute()\n            update.update({\"test/loss\": loss.item()})\n\n    return running_loss / len(dataloader.dataset)\n\n#|%%--%%| <W0oXdaWvA8|iiexgpmwrL>\n\ndevice = torch.device(\"cpu\")\n\ntraining_data = dataset(\"Train\")\ntest_data = dataset(\"Test\")\n\nBATCH_SIZE = 16\nEPOCHS = 16\nLR = 1e-3\n#|%%--%%| <iiexgpmwrL|wqFFBUvlcN>\n\ntrain_loader = torch.utils.data.DataLoader(\n    dataset=training_data, batch_size=BATCH_SIZE, shuffle=True\n)\ntest_loader = torch.utils.data.DataLoader(\n    dataset=test_data, batch_size=BATCH_SIZE, shuffle=True\n)\n#|%%--%%| <wqFFBUvlcN|ROd14kZ0Td>\n\nNUM_LABELS = 3\ntest_metrics = tmetrics.MetricCollection(\n    [\n        tmetrics.classification.MulticlassAccuracy(num_classes=NUM_LABELS),\n    ],\n    prefix=\"test/\",\n)\ntrain_metrics = tmetrics.MetricCollection(\n    [\n        tmetrics.classification.MulticlassAccuracy(num_classes=NUM_LABELS),\n    ],\n    prefix=\"train/\",\n)\n\nmodel = Network()\nmodel.to(device)\n\ntest_tracker = tmetrics.MetricTracker(test_metrics).to(device)\ntrain_tracker = tmetrics.MetricTracker(train_metrics).to(device)\n\nloss_fn = nn.CrossEntropyLoss().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\n\n#|%%--%%| <ROd14kZ0Td|5WyE2whK5p>\n\niterator = tqdm(range(EPOCHS))\nfor t in iterator:\n    train_loss = train_loop(\n        dataloader=train_loader,\n        model=model,\n        loss_fn=loss_fn,\n        optimizer=optimizer,\n        metrics=train_tracker,\n        device=device,\n    )\n\n    metrics = train_tracker.compute()\n    update = {}\n\n    for key in metrics.keys():\n        update[\"epoch/\" + key] = metrics[key]\n\n    update.update({\"epoch\": t, \"epoch/train/loss\": train_loss})\n\n    test_loss = test_loop(\n        dataloader=test_loader,\n        model=model,\n        loss_fn=loss_fn,\n        metrics=test_tracker,\n        device=device,\n    )\n\n    metrics = test_tracker.compute()\n    update = {}\n\n    for key in metrics.keys():\n        update[\"epoch/\" + key] = metrics[key]\n    update.update({\"epoch\": t, \"epoch/test/loss\": test_loss})\n    # iterator.set_postfix(update)\n", "cmd_opts": " -s --md_cell_start=r\\\"\\\"\\\"°°°", "import_complete": 1, "terminal": "kitty"}